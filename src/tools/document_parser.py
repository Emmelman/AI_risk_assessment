# src/tools/document_parser.py
"""
–ü–∞—Ä—Å–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –ò–ò-–∞–≥–µ–Ω—Ç–∞—Ö
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Word (.docx), Excel (.xlsx), PDF –∏ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ–∞–π–ª—ã
"""

import re
import json
from pathlib import Path
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass
from datetime import datetime

# –ò–º–ø–æ—Ä—Ç—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
import docx
from docx.document import Document
import openpyxl
from openpyxl.workbook import Workbook
import PyPDF2
import pdfplumber

from ..utils.logger import get_logger, LogContext


@dataclass
class ParsedDocument:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    file_path: str
    file_type: str
    content: str
    metadata: Dict[str, Any]
    sections: Dict[str, str]
    tables: List[Dict[str, Any]]
    success: bool
    error_message: Optional[str] = None
    parsing_time: float = 0.0


class DocumentParserError(Exception):
    """–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    pass


class BaseDocumentParser:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–µ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.logger = get_logger()
        self.supported_extensions = []
    
    def can_parse(self, file_path: Union[str, Path]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –º–æ–∂–µ—Ç –ª–∏ –ø–∞—Ä—Å–µ—Ä –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ñ–∞–π–ª"""
        path = Path(file_path)
        return path.suffix.lower() in self.supported_extensions
    
    def parse(self, file_path: Union[str, Path]) -> ParsedDocument:
        """–ë–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –≤ –Ω–∞—Å–ª–µ–¥–Ω–∏–∫–∞—Ö)"""
        raise NotImplementedError
    
    def _extract_metadata(self, file_path: Union[str, Path]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞"""
        path = Path(file_path)
        stat = path.stat()
        
        return {
            "file_name": path.name,
            "file_size": stat.st_size,
            "created_time": datetime.fromtimestamp(stat.st_ctime),
            "modified_time": datetime.fromtimestamp(stat.st_mtime),
            "extension": path.suffix.lower()
        }


class WordDocumentParser(BaseDocumentParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Word –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (.docx)"""
    
    def __init__(self):
        super().__init__()
        self.supported_extensions = ['.docx']
    
    def parse(self, file_path: Union[str, Path]) -> ParsedDocument:
        """–ü–∞—Ä—Å–∏–Ω–≥ Word –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        start_time = datetime.now()
        path = Path(file_path)
        
        try:
            with LogContext("parse_word_document", "document_parser", "document_parser"):
                # –û—Ç–∫—Ä—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç
                doc = docx.Document(str(path))
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞–º
                paragraphs = []
                for paragraph in doc.paragraphs:
                    text = paragraph.text.strip()
                    if text:
                        paragraphs.append(text)
                
                content = "\n".join(paragraphs)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Å–µ–∫—Ü–∏–∏
                sections = self._extract_sections_from_word(doc)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
                tables = self._extract_tables_from_word(doc)
                
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                metadata = self._extract_metadata(path)
                metadata.update(self._extract_word_properties(doc))
                
                execution_time = (datetime.now() - start_time).total_seconds()
                
                self.logger.log_document_parsing("document_parser", str(path), "Word", True)
                
                return ParsedDocument(
                    file_path=str(path),
                    file_type="word",
                    content=content,
                    metadata=metadata,
                    sections=sections,
                    tables=tables,
                    success=True,
                    parsing_time=execution_time
                )
                
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            self.logger.log_document_parsing("document_parser", str(path), "Word", False)
            
            return ParsedDocument(
                file_path=str(path),
                file_type="word",
                content="",
                metadata=self._extract_metadata(path),
                sections={},
                tables=[],
                success=False,
                error_message=str(e),
                parsing_time=execution_time
            )
    
    def _extract_sections_from_word(self, doc: Document) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π –∏–∑ Word –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º"""
        sections = {}
        current_section = "introduction"
        current_content = []
        
        for paragraph in doc.paragraphs:
            text = paragraph.text.strip()
            if not text:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
            if self._is_heading(paragraph):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é
                if current_content:
                    sections[current_section] = "\n".join(current_content)
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é
                current_section = self._normalize_section_name(text)
                current_content = []
            else:
                current_content.append(text)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
        if current_content:
            sections[current_section] = "\n".join(current_content)
        
        return sections
    
    def _extract_tables_from_word(self, doc: Document) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –∏–∑ Word –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        tables = []
        
        for i, table in enumerate(doc.tables):
            table_data = {
                "table_index": i,
                "rows": [],
                "headers": []
            }
            
            for row_idx, row in enumerate(table.rows):
                row_data = []
                for cell in row.cells:
                    row_data.append(cell.text.strip())
                
                if row_idx == 0:
                    table_data["headers"] = row_data
                else:
                    table_data["rows"].append(row_data)
            
            tables.append(table_data)
        
        return tables
    
    def _extract_word_properties(self, doc: Document) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤–æ–π—Å—Ç–≤ Word –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        props = {}
        
        try:
            core_props = doc.core_properties
            props.update({
                "title": core_props.title or "",
                "author": core_props.author or "",
                "subject": core_props.subject or "",
                "keywords": core_props.keywords or "",
                "comments": core_props.comments or "",
                "created": core_props.created,
                "modified": core_props.modified
            })
        except Exception:
            pass
        
        return props
    
    def _is_heading(self, paragraph) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º"""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∏–ª—å –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
        if paragraph.style.name.startswith('Heading'):
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (–∂–∏—Ä–Ω—ã–π, —Ä–∞–∑–º–µ—Ä —à—Ä–∏—Ñ—Ç–∞)
        if paragraph.runs:
            first_run = paragraph.runs[0]
            if first_run.bold and len(paragraph.text.strip()) < 100:
                return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        text = paragraph.text.strip()
        heading_patterns = [
            r'^\d+\.\s+',  # "1. –ó–∞–≥–æ–ª–æ–≤–æ–∫"
            r'^[–ê-–ØA-Z][–ê-–ØA-Z\s]+:$',  # "–ó–ê–ì–û–õ–û–í–û–ö:"
            r'^[–ê-–Ø][–∞-—è\s]+:$'  # "–ó–∞–≥–æ–ª–æ–≤–æ–∫:"
        ]
        
        for pattern in heading_patterns:
            if re.match(pattern, text):
                return True
        
        return False
    
    def _normalize_section_name(self, heading_text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏—è —Å–µ–∫—Ü–∏–∏"""
        # –£–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä–∞ –∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        normalized = re.sub(r'^\d+\.?\s*', '', heading_text)
        normalized = re.sub(r'[:\.]$', '', normalized)
        normalized = normalized.lower().strip()
        
        # –ú–∞–ø–ø–∏–Ω–≥ –Ω–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Å–µ–∫—Ü–∏–π
        section_mapping = {
            '–æ–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è': 'general_info',
            '–æ–±—â–∏–µ —Å–≤–µ–¥–µ–Ω–∏—è': 'general_info', 
            '–æ–ø–∏—Å–∞–Ω–∏–µ': 'description',
            '–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ': 'purpose',
            '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è': 'technical_spec',
            '—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏': 'technical_spec',
            '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞': 'architecture',
            '–ø—Ä–æ–º–ø—Ç—ã': 'prompts',
            '–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏': 'instructions',
            '—Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã': 'system_prompts',
            '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è': 'guardrails',
            '–º–µ—Ä—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏': 'guardrails',
            '–±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–∫—Å—Ç': 'business_context',
            '—Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è': 'target_audience',
            '—Ä–∏—Å–∫–∏': 'risks',
            '–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å': 'security',
            '–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏': 'integrations',
            'api': 'integrations',
            '–¥–∞–Ω–Ω—ã–µ': 'data_access',
            '–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ': 'data_access'
        }
        
        return section_mapping.get(normalized, normalized.replace(' ', '_'))


class ExcelDocumentParser(BaseDocumentParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Excel –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (.xlsx)"""
    
    def __init__(self):
        super().__init__()
        self.supported_extensions = ['.xlsx', '.xls']
    
    def parse(self, file_path: Union[str, Path]) -> ParsedDocument:
        """–ü–∞—Ä—Å–∏–Ω–≥ Excel –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        start_time = datetime.now()
        path = Path(file_path)
        
        try:
            with LogContext("parse_excel_document", "document_parser", "document_parser"):
                # –û—Ç–∫—Ä—ã–≤–∞–µ–º Excel —Ñ–∞–π–ª
                workbook = openpyxl.load_workbook(str(path), data_only=True)
                
                content_parts = []
                sections = {}
                tables = []
                
                # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –ª–∏—Å—Ç
                for sheet_name in workbook.sheetnames:
                    sheet = workbook[sheet_name]
                    
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ª–∏—Å—Ç–∞
                    sheet_content, sheet_tables = self._extract_sheet_data(sheet, sheet_name)
                    
                    content_parts.append(f"=== –õ–∏—Å—Ç: {sheet_name} ===\n{sheet_content}")
                    sections[f"sheet_{sheet_name.lower()}"] = sheet_content
                    tables.extend(sheet_tables)
                
                content = "\n\n".join(content_parts)
                
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                metadata = self._extract_metadata(path)
                metadata.update(self._extract_excel_properties(workbook))
                
                execution_time = (datetime.now() - start_time).total_seconds()
                
                self.logger.log_document_parsing("document_parser", str(path), "Excel", True)
                
                return ParsedDocument(
                    file_path=str(path),
                    file_type="excel",
                    content=content,
                    metadata=metadata,
                    sections=sections,
                    tables=tables,
                    success=True,
                    parsing_time=execution_time
                )
                
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            self.logger.log_document_parsing("document_parser", str(path), "Excel", False)
            
            return ParsedDocument(
                file_path=str(path),
                file_type="excel",
                content="",
                metadata=self._extract_metadata(path),
                sections={},
                tables=[],
                success=False,
                error_message=str(e),
                parsing_time=execution_time
            )
    
    def _extract_sheet_data(self, sheet, sheet_name: str) -> tuple[str, List[Dict[str, Any]]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ª–∏—Å—Ç–∞ Excel"""
        content_lines = []
        tables = []
        
        # –ù–∞—Ö–æ–¥–∏–º –≥—Ä–∞–Ω–∏—Ü—ã –¥–∞–Ω–Ω—ã—Ö
        max_row = sheet.max_row
        max_col = sheet.max_column
        
        if max_row == 1 and max_col == 1:
            return "", []
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
        current_table = []
        headers = []
        
        for row_idx, row in enumerate(sheet.iter_rows(max_row=max_row, max_col=max_col), 1):
            row_data = []
            has_data = False
            
            for cell in row:
                value = cell.value
                if value is not None:
                    value = str(value).strip()
                    has_data = True
                else:
                    value = ""
                row_data.append(value)
            
            if has_data:
                # –ü–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ —Å –¥–∞–Ω–Ω—ã–º–∏ - –∑–∞–≥–æ–ª–æ–≤–∫–∏
                if not headers and any(row_data):
                    headers = row_data
                    current_table.append(row_data)
                else:
                    current_table.append(row_data)
                
                # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç—Ä–æ–∫—É –≤ –∫–æ–Ω—Ç–µ–Ω—Ç
                content_lines.append(" | ".join(filter(None, row_data)))
        
        # –°–æ–∑–¥–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –µ—Å–ª–∏ –µ—Å—Ç—å –¥–∞–Ω–Ω—ã–µ
        if current_table:
            tables.append({
                "sheet_name": sheet_name,
                "headers": headers if headers else [],
                "rows": current_table[1:] if headers else current_table,
                "total_rows": len(current_table),
                "total_cols": len(headers) if headers else max_col
            })
        
        content = "\n".join(content_lines)
        return content, tables
    
    def _extract_excel_properties(self, workbook: Workbook) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤–æ–π—Å—Ç–≤ Excel –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        props = {
            "sheet_names": workbook.sheetnames,
            "sheet_count": len(workbook.sheetnames)
        }
        
        try:
            if hasattr(workbook, 'properties'):
                core_props = workbook.properties
                props.update({
                    "title": getattr(core_props, 'title', ''),
                    "author": getattr(core_props, 'creator', ''),
                    "subject": getattr(core_props, 'subject', ''),
                    "created": getattr(core_props, 'created', None),
                    "modified": getattr(core_props, 'modified', None)
                })
        except Exception:
            pass
        
        return props


class PDFDocumentParser(BaseDocumentParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        super().__init__()
        self.supported_extensions = ['.pdf']
    
    def parse(self, file_path: Union[str, Path]) -> ParsedDocument:
        """–ü–∞—Ä—Å–∏–Ω–≥ PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        start_time = datetime.now()
        path = Path(file_path)
        
        try:
            with LogContext("parse_pdf_document", "document_parser", "document_parser"):
                # –ü—Ä–æ–±—É–µ–º pdfplumber –¥–ª—è –ª—É—á—à–µ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
                content_parts = []
                tables = []
                
                with pdfplumber.open(str(path)) as pdf:
                    for page_num, page in enumerate(pdf.pages, 1):
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                        text = page.extract_text()
                        if text:
                            content_parts.append(f"=== –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} ===\n{text}")
                        
                        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —Ç–∞–±–ª–∏—Ü—ã
                        page_tables = page.extract_tables()
                        for table_idx, table in enumerate(page_tables):
                            if table:
                                tables.append({
                                    "page": page_num,
                                    "table_index": table_idx,
                                    "headers": table[0] if table else [],
                                    "rows": table[1:] if len(table) > 1 else [],
                                    "raw_data": table
                                })
                
                content = "\n\n".join(content_parts)
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–µ–∫—Ü–∏–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º
                sections = self._extract_sections_from_text(content)
                
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                metadata = self._extract_metadata(path)
                metadata.update(self._extract_pdf_properties(str(path)))
                
                execution_time = (datetime.now() - start_time).total_seconds()
                
                self.logger.log_document_parsing("document_parser", str(path), "PDF", True)
                
                return ParsedDocument(
                    file_path=str(path),
                    file_type="pdf",
                    content=content,
                    metadata=metadata,
                    sections=sections,
                    tables=tables,
                    success=True,
                    parsing_time=execution_time
                )
                
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            self.logger.log_document_parsing("document_parser", str(path), "PDF", False)
            
            return ParsedDocument(
                file_path=str(path),
                file_type="pdf",
                content="",
                metadata=self._extract_metadata(path),
                sections={},
                tables=[],
                success=False,
                error_message=str(e),
                parsing_time=execution_time
            )
    
    def _extract_sections_from_text(self, text: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ PDF –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º"""
        sections = {}
        lines = text.split('\n')
        
        current_section = "introduction"
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
            if self._is_text_heading(line):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é
                if current_content:
                    sections[current_section] = "\n".join(current_content)
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é
                current_section = self._normalize_section_name(line)
                current_content = []
            else:
                current_content.append(line)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
        if current_content:
            sections[current_section] = "\n".join(current_content)
        
        return sections
    
    def _extract_pdf_properties(self, file_path: str) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–≤–æ–π—Å—Ç–≤ PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        props = {}
        
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                
                props.update({
                    "page_count": len(pdf_reader.pages),
                    "encrypted": pdf_reader.is_encrypted
                })
                
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ PDF
                if pdf_reader.metadata:
                    metadata = pdf_reader.metadata
                    props.update({
                        "title": metadata.get('/Title', ''),
                        "author": metadata.get('/Author', ''),
                        "subject": metadata.get('/Subject', ''),
                        "creator": metadata.get('/Creator', ''),
                        "producer": metadata.get('/Producer', ''),
                        "creation_date": metadata.get('/CreationDate', ''),
                        "modification_date": metadata.get('/ModDate', '')
                    })
        except Exception:
            pass
        
        return props
    
    def _is_text_heading(self, line: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –≤ —Ç–µ–∫—Å—Ç–µ"""
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        heading_patterns = [
            r'^\d+\.\s+',  # "1. –ó–∞–≥–æ–ª–æ–≤–æ–∫"
            r'^[–ê-–ØA-Z][–ê-–ØA-Z\s]+$',  # "–ó–ê–ì–û–õ–û–í–û–ö"
            r'^[–ê-–Ø][–∞-—è\s]+:$',  # "–ó–∞–≥–æ–ª–æ–≤–æ–∫:"
            r'^={3,}\s+.+\s+={3,}$'  # "=== –ó–∞–≥–æ–ª–æ–≤–æ–∫ ==="
        ]
        
        for pattern in heading_patterns:
            if re.match(pattern, line):
                return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Å—Ç—Ä–æ–∫–∏ –≤ –≤–µ—Ä—Ö–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ
        if len(line) < 100 and line.isupper() and len(line.split()) > 1:
            return True
        
        return False


class TextDocumentParser(BaseDocumentParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤ (.txt, .md, .py, .js –∏ —Ç.–¥.)"""
    
    def __init__(self):
        super().__init__()
        self.supported_extensions = ['.txt', '.md', '.py', '.js', '.json', '.yaml', '.yml']
    
    def parse(self, file_path: Union[str, Path]) -> ParsedDocument:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"""
        start_time = datetime.now()
        path = Path(file_path)
        
        try:
            with LogContext("parse_text_document", "document_parser", "document_parser"):
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–¥–∏—Ä–æ–≤–∫—É
                encoding = self._detect_encoding(path)
                
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª
                with open(path, 'r', encoding=encoding) as file:
                    content = file.read()
                
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–µ–∫—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
                sections = self._extract_sections_by_file_type(content, path.suffix.lower())
                
                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                metadata = self._extract_metadata(path)
                metadata.update({
                    "encoding": encoding,
                    "line_count": len(content.splitlines()),
                    "char_count": len(content)
                })
                
                execution_time = (datetime.now() - start_time).total_seconds()
                
                self.logger.log_document_parsing("document_parser", str(path), f"Text({path.suffix})", True)
                
                return ParsedDocument(
                    file_path=str(path),
                    file_type=f"text_{path.suffix[1:]}",
                    content=content,
                    metadata=metadata,
                    sections=sections,
                    tables=[],
                    success=True,
                    parsing_time=execution_time
                )
                
        except Exception as e:
            execution_time = (datetime.now() - start_time).total_seconds()
            self.logger.log_document_parsing("document_parser", str(path), f"Text({path.suffix})", False)
            
            return ParsedDocument(
                file_path=str(path),
                file_type=f"text_{path.suffix[1:]}",
                content="",
                metadata=self._extract_metadata(path),
                sections={},
                tables=[],
                success=False,
                error_message=str(e),
                parsing_time=execution_time
            )
    
    def _detect_encoding(self, path: Path) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ —Ñ–∞–π–ª–∞"""
        # –°–ø–∏—Å–æ–∫ –∫–æ–¥–∏—Ä–æ–≤–æ–∫ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        encodings = ['utf-8', 'cp1251', 'latin-1']
        
        for encoding in encodings:
            try:
                with open(path, 'r', encoding=encoding) as file:
                    file.read()
                return encoding
            except UnicodeDecodeError:
                continue
        
        return 'utf-8'  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
    
    def _extract_sections_by_file_type(self, content: str, extension: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞"""
        sections = {}
        
        if extension == '.md':
            sections = self._extract_markdown_sections(content)
        elif extension in ['.py', '.js']:
            sections = self._extract_code_sections(content, extension)
        elif extension in ['.json', '.yaml', '.yml']:
            sections = self._extract_config_sections(content, extension)
        else:
            # –î–ª—è –æ–±—ã—á–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ–∞–π–ª–æ–≤
            sections = {"content": content}
        
        return sections
    
    def _extract_markdown_sections(self, content: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π –∏–∑ Markdown"""
        sections = {}
        lines = content.split('\n')
        
        current_section = "introduction"
        current_content = []
        
        for line in lines:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ Markdown
            if line.startswith('#'):
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é
                if current_content:
                    sections[current_section] = "\n".join(current_content)
                
                # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é
                heading = line.lstrip('#').strip()
                current_section = self._normalize_section_name(heading)
                current_content = []
            else:
                current_content.append(line)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å–µ–∫—Ü–∏—é
        if current_content:
            sections[current_section] = "\n".join(current_content)
        
        return sections
    
    def _extract_code_sections(self, content: str, extension: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π –∏–∑ –∫–æ–¥–∞"""
        sections = {
            "full_code": content,
            "comments": self._extract_comments(content, extension),
            "functions": self._extract_functions(content, extension),
            "classes": self._extract_classes(content, extension)
        }
        
        return sections
    
    def _extract_config_sections(self, content: str, extension: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–µ–∫—Ü–∏–π –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤"""
        sections = {"config": content}
        
        try:
            if extension == '.json':
                data = json.loads(content)
                sections["parsed_json"] = json.dumps(data, indent=2, ensure_ascii=False)
            # –î–ª—è YAML –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø–∞—Ä—Å–∏–Ω–≥ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        except Exception:
            pass
        
        return sections
    
    def _extract_comments(self, content: str, extension: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –∏–∑ –∫–æ–¥–∞"""
        comments = []
        lines = content.split('\n')
        
        comment_patterns = {
            '.py': [r'^\s*#', r'""".*?"""', r"'''.*?'''"],
            '.js': [r'^\s*//', r'/\*.*?\*/']
        }
        
        patterns = comment_patterns.get(extension, [])
        
        for line in lines:
            for pattern in patterns:
                if re.match(pattern, line.strip()):
                    comments.append(line.strip())
                    break
        
        return "\n".join(comments)
    
    def _extract_functions(self, content: str, extension: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π –∏–∑ –∫–æ–¥–∞"""
        functions = []
        
        function_patterns = {
            '.py': r'^\s*def\s+(\w+)\s*\([^)]*\):',
            '.js': r'^\s*function\s+(\w+)\s*\([^)]*\)\s*{|^\s*const\s+(\w+)\s*=\s*\([^)]*\)\s*=>'
        }
        
        pattern = function_patterns.get(extension)
        if pattern:
            for match in re.finditer(pattern, content, re.MULTILINE):
                functions.append(match.group(0))
        
        return "\n".join(functions)
    
    def _extract_classes(self, content: str, extension: str) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –∫–æ–¥–∞"""
        classes = []
        
        class_patterns = {
            '.py': r'^\s*class\s+(\w+).*?:',
            '.js': r'^\s*class\s+(\w+).*?{'
        }
        
        pattern = class_patterns.get(extension)
        if pattern:
            for match in re.finditer(pattern, content, re.MULTILINE):
                classes.append(match.group(0))
        
        return "\n".join(classes)


# ===============================
# –ì–ª–∞–≤–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
# ===============================

class DocumentParser:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    def __init__(self):
        self.parsers = [
            WordDocumentParser(),
            ExcelDocumentParser(),
            PDFDocumentParser(),
            TextDocumentParser()
        ]
        self.logger = get_logger()
    
    def can_parse(self, file_path: Union[str, Path]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –º–æ–∂–µ—Ç –ª–∏ —Å–∏—Å—Ç–µ–º–∞ –ø–∞—Ä—Å–∏—Ç—å —Ñ–∞–π–ª"""
        return any(parser.can_parse(file_path) for parser in self.parsers)
    
    def parse_document(self, file_path: Union[str, Path]) -> ParsedDocument:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º –≤—ã–±–æ—Ä–æ–º –ø–∞—Ä—Å–µ—Ä–∞"""
        path = Path(file_path)
        
        if not path.exists():
            return ParsedDocument(
                file_path=str(path),
                file_type="unknown",
                content="",
                metadata={},
                sections={},
                tables=[],
                success=False,
                error_message=f"–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}"
            )
        
        # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–π –ø–∞—Ä—Å–µ—Ä
        for parser in self.parsers:
            if parser.can_parse(path):
                bound_logger = self.logger.bind_context("document_parser", "document_parser")
                bound_logger.info(f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞: {path.name} ({parser.__class__.__name__})")
                
                result = parser.parse(path)
                
                if result.success:
                    bound_logger.info(f"‚úÖ –§–∞–π–ª —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω: {len(result.content)} —Å–∏–º–≤–æ–ª–æ–≤, {len(result.sections)} —Å–µ–∫—Ü–∏–π")
                else:
                    bound_logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {result.error_message}")
                
                return result
        
        # –ï—Å–ª–∏ –ø–∞—Ä—Å–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω
        return ParsedDocument(
            file_path=str(path),
            file_type="unsupported",
            content="",
            metadata=self._extract_basic_metadata(path),
            sections={},
            tables=[],
            success=False,
            error_message=f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {path.suffix}"
        )
    
    def parse_multiple_documents(self, file_paths: List[Union[str, Path]]) -> List[ParsedDocument]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        results = []
        
        for file_path in file_paths:
            result = self.parse_document(file_path)
            results.append(result)
        
        # –õ–æ–≥–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        successful = sum(1 for r in results if r.success)
        failed = len(results) - successful
        
        bound_logger = self.logger.bind_context("document_parser", "document_parser")
        bound_logger.info(f"üìä –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω: {successful} —É—Å–ø–µ—à–Ω–æ, {failed} –æ—à–∏–±–æ–∫")
        
        return results
    
    def extract_agent_info_from_documents(self, documents: List[ParsedDocument]) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∞–≥–µ–Ω—Ç–µ –∏–∑ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∞–≥–µ–Ω—Ç–æ–≤ –ò–ò
        """
        agent_info = {
            "name": "",
            "description": "",
            "technical_specs": {},
            "prompts": [],
            "guardrails": [],
            "business_context": {},
            "source_files": []
        }
        
        for doc in documents:
            if not doc.success:
                continue
            
            agent_info["source_files"].append(doc.file_path)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Å–µ–∫—Ü–∏–π
            for section_name, section_content in doc.sections.items():
                self._extract_info_from_section(agent_info, section_name, section_content)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ç–∞–±–ª–∏—Ü
            for table in doc.tables:
                self._extract_info_from_table(agent_info, table)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            if "title" in doc.metadata and doc.metadata["title"]:
                if not agent_info["name"]:
                    agent_info["name"] = doc.metadata["title"]
        
        return agent_info
    
    def _extract_info_from_section(self, agent_info: Dict[str, Any], section_name: str, content: str):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Å–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        content = content.strip()
        if not content:
            return
        
        # –ú–∞–ø–ø–∏–Ω–≥ —Å–µ–∫—Ü–∏–π –Ω–∞ –ø–æ–ª—è –∞–≥–µ–Ω—Ç–∞
        if section_name in ["general_info", "description", "introduction"]:
            if not agent_info["description"]:
                agent_info["description"] = content
        
        elif section_name in ["technical_spec", "architecture"]:
            agent_info["technical_specs"][section_name] = content
        
        elif section_name in ["prompts", "system_prompts", "instructions"]:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞
            prompts = self._extract_prompts_from_text(content)
            agent_info["prompts"].extend(prompts)
        
        elif section_name in ["guardrails", "security", "limitations"]:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
            guardrails = self._extract_guardrails_from_text(content)
            agent_info["guardrails"].extend(guardrails)
        
        elif section_name in ["business_context", "target_audience"]:
            agent_info["business_context"][section_name] = content
    
    def _extract_info_from_table(self, agent_info: Dict[str, Any], table: Dict[str, Any]):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã"""
        if not table.get("headers") or not table.get("rows"):
            return
        
        headers = [h.lower().strip() if h else "" for h in table["headers"]]
        
        # –ò—â–µ–º —Ç–∞–±–ª–∏—Ü—ã —Å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏
        tech_indicators = ["–ø–∞—Ä–∞–º–µ—Ç—Ä", "—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞", "–∑–Ω–∞—á–µ–Ω–∏–µ", "–æ–ø–∏—Å–∞–Ω–∏–µ"]
        if any(indicator in " ".join(headers) for indicator in tech_indicators):
            for row in table["rows"]:
                if len(row) >= 2 and row[0] and row[1]:
                    key = str(row[0]).strip()
                    value = str(row[1]).strip()
                    agent_info["technical_specs"][key] = value
    
    def _extract_prompts_from_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        prompts = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤
        prompt_patterns = [
            r'–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç[:\s]*(.+?)(?=\n\n|\n[–ê-–Ø]|$)',
            r'–ü—Ä–æ–º–ø—Ç[:\s]*(.+?)(?=\n\n|\n[–ê-–Ø]|$)',
            r'–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è[:\s]*(.+?)(?=\n\n|\n[–ê-–Ø]|$)',
            r'"([^"]+)"',  # –¢–µ–∫—Å—Ç –≤ –∫–∞–≤—ã—á–∫–∞—Ö
            r'```\s*(.+?)\s*```'  # –ö–æ–¥ –±–ª–æ–∫–∏
        ]
        
        for pattern in prompt_patterns:
            matches = re.findall(pattern, text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                cleaned = match.strip()
                if len(cleaned) > 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø—Ä–æ–º–ø—Ç–∞
                    prompts.append(cleaned)
        
        # –†–∞–∑–±–∏–≤–∞–µ–º –ø–æ —Å—Ç—Ä–æ–∫–∞–º –µ—Å–ª–∏ –Ω–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        if not prompts:
            lines = text.split('\n')
            for line in lines:
                line = line.strip()
                if len(line) > 20 and not line.startswith('‚Ä¢') and not line.startswith('-'):
                    prompts.append(line)
        
        return prompts
    
    def _extract_guardrails_from_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        guardrails = []
        
        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
        lines = text.split('\n')
        for line in lines:
            line = line.strip()
            
            # –ò—â–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º–∏
            restriction_indicators = [
                '–Ω–µ –¥–æ–ª–∂–µ–Ω', '–∑–∞–ø—Ä–µ—â–µ–Ω–æ', '–Ω–µ–ª—å–∑—è', '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ',
                '–Ω–µ —Ä–∞–∑—Ä–µ—à–∞–µ—Ç—Å—è', '–Ω–µ–¥–æ–ø—É—Å—Ç–∏–º–æ', '–∏—Å–∫–ª—é—á–∏—Ç—å'
            ]
            
            if any(indicator in line.lower() for indicator in restriction_indicators):
                if len(line) > 10:
                    guardrails.append(line)
            
            # –ò—â–µ–º —Å–ø–∏—Å–∫–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π
            if line.startswith(('‚Ä¢', '-', '*')) and len(line) > 10:
                guardrails.append(line[1:].strip())
        
        return guardrails
    
    def _extract_basic_metadata(self, path: Path) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–∞"""
        try:
            stat = path.stat()
            return {
                "file_name": path.name,
                "file_size": stat.st_size,
                "extension": path.suffix.lower(),
                "created_time": datetime.fromtimestamp(stat.st_ctime),
                "modified_time": datetime.fromtimestamp(stat.st_mtime)
            }
        except Exception:
            return {"file_name": path.name, "extension": path.suffix.lower()}
    
    def get_supported_extensions(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π"""
        extensions = []
        for parser in self.parsers:
            extensions.extend(parser.supported_extensions)
        return sorted(list(set(extensions)))
    
    def get_parsing_stats(self, documents: List[ParsedDocument]) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–∞—Ä—Å–∏–Ω–≥–∞"""
        if not documents:
            return {}
        
        successful = [d for d in documents if d.success]
        failed = [d for d in documents if not d.success]
        
        stats = {
            "total_documents": len(documents),
            "successful": len(successful),
            "failed": len(failed),
            "success_rate": len(successful) / len(documents) * 100,
            "total_content_length": sum(len(d.content) for d in successful),
            "total_sections": sum(len(d.sections) for d in successful),
            "total_tables": sum(len(d.tables) for d in successful),
            "avg_parsing_time": sum(d.parsing_time for d in documents) / len(documents),
            "file_types": {}
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º —Ñ–∞–π–ª–æ–≤
        for doc in documents:
            file_type = doc.file_type
            if file_type not in stats["file_types"]:
                stats["file_types"][file_type] = {"count": 0, "successful": 0}
            
            stats["file_types"][file_type]["count"] += 1
            if doc.success:
                stats["file_types"][file_type]["successful"] += 1
        
        return stats


# ===============================
# –£—Ç–∏–ª–∏—Ç–∞—Ä–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
# ===============================

def create_document_parser() -> DocumentParser:
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–∞—Ä—Å–µ—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    return DocumentParser()


def parse_agent_documents(
    file_paths: List[Union[str, Path]], 
    extract_agent_info: bool = True
) -> tuple[List[ParsedDocument], Optional[Dict[str, Any]]]:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∞–≥–µ–Ω—Ç–∞
    
    Args:
        file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
        extract_agent_info: –ò–∑–≤–ª–µ–∫–∞—Ç—å –ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–≥–µ–Ω—Ç–µ
    
    Returns:
        Tuple –∏–∑ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–∞—Ä—Å–∏–Ω–≥–∞, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∞–≥–µ–Ω—Ç–µ)
    """
    parser = create_document_parser()
    
    # –ü–∞—Ä—Å–∏–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    parsed_docs = parser.parse_multiple_documents(file_paths)
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∞–≥–µ–Ω—Ç–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    agent_info = None
    if extract_agent_info:
        agent_info = parser.extract_agent_info_from_documents(parsed_docs)
    
    return parsed_docs, agent_info


def get_document_summary(parsed_doc: ParsedDocument) -> Dict[str, Any]:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–π —Å–≤–æ–¥–∫–∏ –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ"""
    return {
        "file_name": Path(parsed_doc.file_path).name,
        "file_type": parsed_doc.file_type,
        "success": parsed_doc.success,
        "content_length": len(parsed_doc.content),
        "sections_count": len(parsed_doc.sections),
        "tables_count": len(parsed_doc.tables),
        "parsing_time": parsed_doc.parsing_time,
        "error": parsed_doc.error_message if not parsed_doc.success else None
    }


# –≠–∫—Å–ø–æ—Ä—Ç –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤ –∏ —Ñ—É–Ω–∫—Ü–∏–π
__all__ = [
    "DocumentParser",
    "ParsedDocument", 
    "DocumentParserError",
    "WordDocumentParser",
    "ExcelDocumentParser", 
    "PDFDocumentParser",
    "TextDocumentParser",
    "create_document_parser",
    "parse_agent_documents",
    "get_document_summary"
]