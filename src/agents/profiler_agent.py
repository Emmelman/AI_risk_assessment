# src/agents/profiler_agent.py
"""
–ü—Ä–æ—Ñ–∞–π–ª–µ—Ä-–∞–≥–µ–Ω—Ç –¥–ª—è —Å–±–æ—Ä–∞ –∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –æ–± –ò–ò-–∞–≥–µ–Ω—Ç–µ
–°–æ–±–∏—Ä–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã, –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏, –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
"""

import json
from pathlib import Path
from typing import Dict, Any, List, Optional, Union
from datetime import datetime

from .base_agent import AnalysisAgent, AgentConfig
from ..models.risk_models import AgentProfile, AgentTaskResult, ProcessingStatus, AgentType, AutonomyLevel, DataSensitivity
from ..tools.document_parser import create_document_parser, parse_agent_documents
from ..tools.code_analyzer import create_code_analyzer, analyze_agent_codebase
from ..tools.prompt_analyzer import create_prompt_analyzer, analyze_agent_prompts
from ..utils.logger import LogContext


class ProfilerAgent(AnalysisAgent):
    """
    –ê–≥–µ–Ω—Ç-–ø—Ä–æ—Ñ–∞–π–ª–µ—Ä –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ–± –ò–ò-–∞–≥–µ–Ω—Ç–µ
    
    –§—É–Ω–∫—Ü–∏–∏:
    1. –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ (Word, Excel, PDF)
    2. –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã (Python, JavaScript, Java)
    3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π
    4. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫
    5. –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è –∞–≥–µ–Ω—Ç–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–æ–≤
    """
    
    def __init__(self, config: AgentConfig):
        super().__init__(config)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        self.document_parser = create_document_parser()
        self.code_analyzer = create_code_analyzer()
        self.prompt_analyzer = create_prompt_analyzer()
    
    def get_system_prompt(self) -> str:
        """–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ø—Ä–æ—Ñ–∞–π–ª–µ—Ä–∞"""
        return """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—é –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω—ã—Ö —Ä–∏—Å–∫–æ–≤.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞: —Å–æ–∑–¥–∞–≤–∞—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ—Ñ–∏–ª–∏ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ—Å—Ç—É–ø–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

–ü–†–ò–ù–¶–ò–ü–´ –ê–ù–ê–õ–ò–ó–ê:
1. –¢—â–∞—Ç–µ–ª—å–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –≤—Å–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
2. –û–ø—Ä–µ–¥–µ–ª—è–π –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞–≥–µ–Ω—Ç–∞
3. –í—ã—è–≤–ª—è–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
4. –û—Ü–µ–Ω–∏–≤–∞–π —É—Ä–æ–≤–µ–Ω—å –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏
5. –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–π –¥–æ—Å—Ç—É–ø –∫ –¥–∞–Ω–Ω—ã–º
6. –ò–∑–≤–ª–µ–∫–∞–π –∫–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–º–ø—Ç—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

–§–û–†–ú–ê–¢ –í–´–í–û–î–ê: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π JSON —Å –ø–æ–ª–Ω—ã–º –ø—Ä–æ—Ñ–∏–ª–µ–º –∞–≥–µ–Ω—Ç–∞

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ü–û–õ–Ø:
- name: –Ω–∞–∑–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞
- agent_type: —Ç–∏–ø –∞–≥–µ–Ω—Ç–∞ (chatbot, assistant, trader, scorer, analyzer, generator)
- autonomy_level: —É—Ä–æ–≤–µ–Ω—å –∞–≤—Ç–æ–Ω–æ–º–Ω–æ—Å—Ç–∏ (manual, assisted, supervised, autonomous)
- data_access: —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö (public, internal, confidential, critical)
- target_audience: —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è
- llm_model: –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å
- system_prompts: —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
- guardrails: –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

–ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –æ–±—ä–µ–∫—Ç–∏–≤–Ω—ã–º –≤ –∞–Ω–∞–ª–∏–∑–µ."""
    
    async def process(
        self, 
        input_data: Dict[str, Any], 
        assessment_id: str
    ) -> AgentTaskResult:
        """
        –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ—Ñ–∞–π–ª–∏–Ω–≥–∞ –∞–≥–µ–Ω—Ç–∞
        
        Args:
            input_data: –°–æ–¥–µ—Ä–∂–∏—Ç –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –∏ –ø–∞–ø–∫–∞–º –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                - source_files: List[str] - —Å–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤/–ø–∞–ø–æ–∫
                - agent_name: Optional[str] - –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –∏–º—è –∞–≥–µ–Ω—Ç–∞
            assessment_id: ID –æ—Ü–µ–Ω–∫–∏
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç —Å –ø—Ä–æ—Ñ–∏–ª–µ–º –∞–≥–µ–Ω—Ç–∞
        """
        start_time = datetime.now()
        
        try:
            with LogContext("profile_agent", assessment_id, self.name):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                source_files = input_data.get("source_files", [])
                preliminary_name = input_data.get("agent_name", "Unknown_Agent")
                
                if not source_files:
                    raise ValueError("–ù–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã —Ñ–∞–π–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                
                # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                collected_data = await self._collect_all_data(source_files, assessment_id)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é LLM
                agent_profile = await self._analyze_and_create_profile(
                    collected_data, preliminary_name, assessment_id
                )
                
                # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                end_time = datetime.now()
                execution_time = (end_time - start_time).total_seconds()
                
                return AgentTaskResult(
                    agent_name=self.name,
                    task_type="profiling",
                    status=ProcessingStatus.COMPLETED,
                    result_data={
                        "agent_profile": agent_profile.dict(),
                        "collected_data_summary": self._create_data_summary(collected_data)
                    },
                    start_time=start_time,
                    end_time=end_time,
                    execution_time_seconds=execution_time
                )
                
        except Exception as e:
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            
            return AgentTaskResult(
                agent_name=self.name,
                task_type="profiling",
                status=ProcessingStatus.FAILED,
                error_message=str(e),
                start_time=start_time,
                end_time=end_time,
                execution_time_seconds=execution_time
            )
    
    async def _collect_all_data(
        self, 
        source_files: List[str], 
        assessment_id: str
    ) -> Dict[str, Any]:
        """–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        
        collected_data = {
            "documents": [],
            "code_analysis": None,
            "prompt_analysis": None,
            "source_files": source_files,
            "errors": []
        }
        
        # –†–∞–∑–¥–µ–ª—è–µ–º —Ñ–∞–π–ª—ã –∏ –ø–∞–ø–∫–∏
        files_to_parse = []
        directories_to_analyze = []
        
        for source in source_files:
            path = Path(source)
            if path.is_file():
                files_to_parse.append(path)
            elif path.is_dir():
                directories_to_analyze.append(path)
                # –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª—ã –∏–∑ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
                for file_path in path.rglob('*'):
                    if file_path.is_file():
                        files_to_parse.append(file_path)
        
        # 1. –ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.logger.bind_context(assessment_id, self.name).info(
            f"üìÑ –ü–∞—Ä—Å–∏–Ω–≥ {len(files_to_parse)} —Ñ–∞–π–ª–æ–≤"
        )
        
        try:
            parsed_docs, agent_info = parse_agent_documents(
                files_to_parse, 
                extract_agent_info=True
            )
            
            collected_data["documents"] = [
                {
                    "file_path": doc.file_path,
                    "file_type": doc.file_type,
                    "content": doc.content,
                    "sections": doc.sections,
                    "tables": doc.tables,
                    "success": doc.success
                }
                for doc in parsed_docs
            ]
            
            collected_data["document_agent_info"] = agent_info
            
        except Exception as e:
            collected_data["errors"].append(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        
        # 2. –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–æ–≤–æ–π –±–∞–∑—ã
        self.logger.bind_context(assessment_id, self.name).info(
            f"üíª –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞ –≤ {len(directories_to_analyze)} –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö"
        )
        
        for directory in directories_to_analyze:
            try:
                code_analysis = analyze_agent_codebase(directory, max_files=50)
                
                if code_analysis.success:
                    collected_data["code_analysis"] = {
                        "project_path": code_analysis.project_path,
                        "total_files": code_analysis.total_files,
                        "total_lines": code_analysis.total_lines,
                        "languages": code_analysis.languages,
                        "dependencies": code_analysis.dependencies,
                        "entry_points": code_analysis.entry_points,
                        "security_summary": code_analysis.security_summary,
                        "complexity_summary": code_analysis.complexity_summary
                    }
                    break  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —É—Å–ø–µ—à–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
                
            except Exception as e:
                collected_data["errors"].append(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞ {directory}: {e}")
        
        # 3. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–º–ø—Ç–æ–≤
        self.logger.bind_context(assessment_id, self.name).info(
            "üîç –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π"
        )
        
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–º–ø—Ç–æ–≤
            prompt_sources = []
            
            # –ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            for doc in collected_data["documents"]:
                if doc["success"]:
                    # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–µ–∫—Ü–∏–π —Å –ø—Ä–æ–º–ø—Ç–∞–º–∏
                    for section_name, section_content in doc["sections"].items():
                        if any(keyword in section_name.lower() for keyword in 
                               ['prompt', 'instruction', 'system', 'guardrail']):
                            prompt_sources.append(section_content)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –µ—Å–ª–∏ –æ–Ω –Ω–µ–±–æ–ª—å—à–æ–π
                    if len(doc["content"]) < 5000:
                        prompt_sources.append(doc["content"])
            
            # –ò–∑ –∫–æ–¥–∞ (–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –∏ —Å—Ç—Ä–æ–∫–∏)
            if collected_data["code_analysis"]:
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∞–º–∏ —Ñ–∞–π–ª—ã –∫–æ–¥–∞
                for file_path in files_to_parse:
                    if file_path.suffix.lower() in ['.py', '.js', '.java']:
                        prompt_sources.append(str(file_path))
            
            if prompt_sources:
                prompt_analysis = analyze_agent_prompts(prompt_sources)
                
                if prompt_analysis.success:
                    collected_data["prompt_analysis"] = {
                        "total_prompts": prompt_analysis.total_prompts,
                        "system_prompts": [p.content for p in prompt_analysis.system_prompts],
                        "guardrails": [p.content for p in prompt_analysis.guardrails],
                        "capabilities": prompt_analysis.capabilities,
                        "personality_traits": prompt_analysis.personality_traits,
                        "restrictions": prompt_analysis.restrictions,
                        "risk_indicators": prompt_analysis.risk_indicators,
                        "complexity_score": prompt_analysis.complexity_score
                    }
                
        except Exception as e:
            collected_data["errors"].append(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–º–ø—Ç–æ–≤: {e}")
        
        return collected_data
    
    async def _analyze_and_create_profile(
        self,
        collected_data: Dict[str, Any],
        preliminary_name: str,
        assessment_id: str
    ) -> AgentProfile:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è –∞–≥–µ–Ω—Ç–∞"""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ LLM
        analysis_data = self._format_data_for_llm(collected_data)
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è
        extraction_prompt = """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ–± –ò–ò-–∞–≥–µ–Ω—Ç–µ –∏ —Å–æ–∑–¥–∞–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å.

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–´–ï –ü–û–õ–Ø –í JSON:
{
    "name": "string - –Ω–∞–∑–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞",
    "version": "string - –≤–µ—Ä—Å–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1.0)",
    "description": "string - –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è –∞–≥–µ–Ω—Ç–∞",
    "agent_type": "string - –æ–¥–∏–Ω –∏–∑: chatbot, assistant, trader, scorer, analyzer, generator, other",
    "llm_model": "string - –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è LLM –º–æ–¥–µ–ª—å",
    "autonomy_level": "string - –æ–¥–∏–Ω –∏–∑: manual, assisted, supervised, autonomous",
    "data_access": ["array of strings - —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö: public, internal, confidential, critical"],
    "external_apis": ["array of strings - –≤–Ω–µ—à–Ω–∏–µ API"],
    "target_audience": "string - —Ü–µ–ª–µ–≤–∞—è –∞—É–¥–∏—Ç–æ—Ä–∏—è",
    "operations_per_hour": "number or null - –æ–ø–µ—Ä–∞—Ü–∏–π –≤ —á–∞—Å",
    "revenue_per_operation": "number or null - –¥–æ—Ö–æ–¥ —Å –æ–ø–µ—Ä–∞—Ü–∏–∏ –≤ —Ä—É–±–ª—è—Ö",
    "system_prompts": ["array of strings - —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã"],
    "guardrails": ["array of strings - –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏"]
}

–ü–†–ê–í–ò–õ–ê –ê–ù–ê–õ–ò–ó–ê:
1. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–π —Ä–∞–∑—É–º–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
2. –î–ª—è agent_type –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –∏ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ
3. –î–ª—è autonomy_level –æ—Ü–µ–Ω–∏–≤–∞–π —Å—Ç–µ–ø–µ–Ω—å —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
4. –î–ª—è data_access –æ–ø—Ä–µ–¥–µ–ª—è–π –ø–æ —Ç–∏–ø–∞–º –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
5. –ò–∑–≤–ª–µ–∫–∞–π –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è

–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤."""
        
        # –í—ã–∑—ã–≤–∞–µ–º LLM –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è
        llm_result = await self.call_llm_structured(
            data_to_analyze=analysis_data,
            extraction_prompt=extraction_prompt,
            assessment_id=assessment_id,
            expected_format="JSON"
        )
        
        # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç AgentProfile
        profile_data = self._validate_and_fix_profile_data(llm_result, preliminary_name)
        
        agent_profile = AgentProfile(
            name=profile_data["name"],
            version=profile_data.get("version", "1.0"),
            description=profile_data["description"],
            agent_type=AgentType(profile_data["agent_type"]),
            llm_model=profile_data["llm_model"],
            autonomy_level=AutonomyLevel(profile_data["autonomy_level"]),
            data_access=[DataSensitivity(da) for da in profile_data.get("data_access", [])],
            external_apis=profile_data.get("external_apis", []),
            target_audience=profile_data["target_audience"],
            operations_per_hour=profile_data.get("operations_per_hour"),
            revenue_per_operation=profile_data.get("revenue_per_operation"),
            system_prompts=profile_data.get("system_prompts", []),
            guardrails=profile_data.get("guardrails", []),
            source_files=collected_data.get("source_files", [])
        )
        
        return agent_profile
    
    def _format_data_for_llm(self, collected_data: Dict[str, Any]) -> str:
        """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ LLM"""
        
        formatted_parts = []
        
        # 1. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if collected_data.get("documents"):
            formatted_parts.append("=== –î–û–ö–£–ú–ï–ù–¢–ê–¶–ò–Ø ===")
            
            for doc in collected_data["documents"]:
                if doc["success"]:
                    formatted_parts.append(f"\n–§–∞–π–ª: {Path(doc['file_path']).name}")
                    formatted_parts.append(f"–¢–∏–ø: {doc['file_type']}")
                    
                    # –°–µ–∫—Ü–∏–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                    for section_name, section_content in doc["sections"].items():
                        if section_content.strip():
                            formatted_parts.append(f"\n[{section_name.upper()}]")
                            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Å–µ–∫—Ü–∏–∏
                            content = section_content[:1000] + "..." if len(section_content) > 1000 else section_content
                            formatted_parts.append(content)
                    
                    # –¢–∞–±–ª–∏—Ü—ã
                    if doc["tables"]:
                        formatted_parts.append(f"\n–¢–∞–±–ª–∏—Ü –Ω–∞–π–¥–µ–Ω–æ: {len(doc['tables'])}")
        
        # 2. –ê–Ω–∞–ª–∏–∑ –∫–æ–¥–∞
        if collected_data.get("code_analysis"):
            code_data = collected_data["code_analysis"]
            formatted_parts.append("\n\n=== –ê–ù–ê–õ–ò–ó –ö–û–î–ê ===")
            formatted_parts.append(f"–ü—Ä–æ–µ–∫—Ç: {code_data['project_path']}")
            formatted_parts.append(f"–§–∞–π–ª–æ–≤: {code_data['total_files']}")
            formatted_parts.append(f"–°—Ç—Ä–æ–∫ –∫–æ–¥–∞: {code_data['total_lines']}")
            formatted_parts.append(f"–Ø–∑—ã–∫–∏: {', '.join(code_data['languages'].keys())}")
            
            if code_data.get("dependencies"):
                formatted_parts.append(f"\n–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:")
                dep_count = 0
                for file_path, deps in code_data["dependencies"].items():
                    if dep_count < 20:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                        formatted_parts.append(f"  {Path(file_path).name}: {', '.join(deps[:5])}")
                        dep_count += len(deps)
            
            if code_data.get("entry_points"):
                formatted_parts.append(f"\n–¢–æ—á–∫–∏ –≤—Ö–æ–¥–∞: {', '.join(code_data['entry_points'])}")
            
            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å
            security = code_data.get("security_summary", {})
            complexity = code_data.get("complexity_summary", {})
            
            formatted_parts.append(f"\n–ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å: {security.get('total_issues', 0)} –ø—Ä–æ–±–ª–µ–º")
            formatted_parts.append(f"–°—Ä–µ–¥–Ω—è—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å: {complexity.get('average_complexity', 0):.1f}")
        
        # 3. –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–º–ø—Ç–æ–≤
        if collected_data.get("prompt_analysis"):
            prompt_data = collected_data["prompt_analysis"]
            formatted_parts.append("\n\n=== –ê–ù–ê–õ–ò–ó –ü–†–û–ú–ü–¢–û–í ===")
            formatted_parts.append(f"–í—Å–µ–≥–æ –ø—Ä–æ–º–ø—Ç–æ–≤: {prompt_data['total_prompts']}")
            
            if prompt_data.get("system_prompts"):
                formatted_parts.append(f"\n–°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã:")
                for i, prompt in enumerate(prompt_data["system_prompts"][:3]):  # –ü–µ—Ä–≤—ã–µ 3
                    formatted_parts.append(f"  {i+1}. {prompt[:200]}...")
            
            if prompt_data.get("guardrails"):
                formatted_parts.append(f"\n–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:")
                for i, guardrail in enumerate(prompt_data["guardrails"][:3]):
                    formatted_parts.append(f"  {i+1}. {guardrail[:200]}...")
            
            if prompt_data.get("capabilities"):
                formatted_parts.append(f"\n–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏: {', '.join(prompt_data['capabilities'])}")
            
            if prompt_data.get("risk_indicators"):
                formatted_parts.append(f"–ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ä–∏—Å–∫–∞: {', '.join(prompt_data['risk_indicators'])}")
        
        # 4. –û—à–∏–±–∫–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if collected_data.get("errors"):
            formatted_parts.append(f"\n\n=== –û–®–ò–ë–ö–ò –°–ë–û–†–ê –î–ê–ù–ù–´–• ===")
            for error in collected_data["errors"]:
                formatted_parts.append(f"- {error}")
        
        return "\n".join(formatted_parts)
    
    def _validate_and_fix_profile_data(
        self, 
        llm_result: Dict[str, Any], 
        preliminary_name: str
    ) -> Dict[str, Any]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ—Ñ–∏–ª—è –æ—Ç LLM"""
        
        # –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        defaults = {
            "name": preliminary_name,
            "version": "1.0",
            "description": "–ò–ò-–∞–≥–µ–Ω—Ç (–æ–ø–∏—Å–∞–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ)",
            "agent_type": "other",
            "llm_model": "unknown",
            "autonomy_level": "supervised",
            "data_access": ["internal"],
            "external_apis": [],
            "target_audience": "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å–∏—Å—Ç–µ–º—ã",
            "operations_per_hour": None,
            "revenue_per_operation": None,
            "system_prompts": [],
            "guardrails": []
        }
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        for key, default_value in defaults.items():
            if key not in llm_result or llm_result[key] is None:
                llm_result[key] = default_value
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –µ–Ω—É–º–æ–≤
        valid_agent_types = [e.value for e in AgentType]
        if llm_result["agent_type"] not in valid_agent_types:
            llm_result["agent_type"] = "other"
        
        valid_autonomy_levels = [e.value for e in AutonomyLevel]
        if llm_result["autonomy_level"] not in valid_autonomy_levels:
            llm_result["autonomy_level"] = "supervised"
        
        valid_data_sensitivities = [e.value for e in DataSensitivity]
        validated_data_access = []
        for da in llm_result.get("data_access", []):
            if da in valid_data_sensitivities:
                validated_data_access.append(da)
        if not validated_data_access:
            validated_data_access = ["internal"]
        llm_result["data_access"] = validated_data_access
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–ø–∏—Å–∫–æ–≤
        for list_field in ["external_apis", "system_prompts", "guardrails"]:
            if not isinstance(llm_result.get(list_field), list):
                llm_result[list_field] = []
        
        return llm_result
    
    def _create_data_summary(self, collected_data: Dict[str, Any]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–π —Å–≤–æ–¥–∫–∏ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        
        summary = {
            "documents_processed": 0,
            "documents_successful": 0,
            "code_analysis_success": False,
            "prompt_analysis_success": False,
            "total_source_files": len(collected_data.get("source_files", [])),
            "errors_count": len(collected_data.get("errors", []))
        }
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        if collected_data.get("documents"):
            summary["documents_processed"] = len(collected_data["documents"])
            summary["documents_successful"] = sum(
                1 for doc in collected_data["documents"] if doc["success"]
            )
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–¥–∞
        if collected_data.get("code_analysis"):
            summary["code_analysis_success"] = True
            summary["total_code_files"] = collected_data["code_analysis"]["total_files"]
            summary["total_code_lines"] = collected_data["code_analysis"]["total_lines"]
            summary["programming_languages"] = list(collected_data["code_analysis"]["languages"].keys())
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–º–ø—Ç–æ–≤
        if collected_data.get("prompt_analysis"):
            summary["prompt_analysis_success"] = True
            summary["total_prompts_found"] = collected_data["prompt_analysis"]["total_prompts"]
            summary["system_prompts_found"] = len(collected_data["prompt_analysis"]["system_prompts"])
            summary["guardrails_found"] = len(collected_data["prompt_analysis"]["guardrails"])
        
        return summary
    
    def _get_required_result_fields(self) -> List[str]:
        """–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ø—Ä–æ—Ñ–∞–π–ª–µ—Ä–∞"""
        return ["agent_profile", "collected_data_summary"]
   
    async def run(self, input_data: Dict[str, Any], assessment_id: str) -> AgentTaskResult:
        """
        –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∞–≥–µ–Ω—Ç–∞ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
        """
        start_time = datetime.now()
        
        try:
            with LogContext("profile_agent", assessment_id, self.name):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                source_files = input_data.get("source_files", [])
                preliminary_name = input_data.get("agent_name", "Unknown_Agent")
                
                if not source_files:
                    raise ValueError("–ù–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω—ã —Ñ–∞–π–ª—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
                
                # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                collected_data = await self._collect_all_data(source_files, assessment_id)
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å –ø–æ–º–æ—â—å—é LLM
                agent_profile = await self._analyze_and_create_profile(
                    collected_data, preliminary_name, assessment_id
                )
                
                # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ë–ï–ó RiskEvaluation
                end_time = datetime.now()
                execution_time = (end_time - start_time).total_seconds()
                
                return AgentTaskResult(
                    agent_name=self.name,
                    task_type="profiling",
                    status=ProcessingStatus.COMPLETED,
                    result_data={
                        "agent_profile": agent_profile.dict(),
                        "collected_data_summary": self._create_data_summary(collected_data)
                    },
                    start_time=start_time,
                    end_time=end_time,
                    execution_time_seconds=execution_time
                )
                
        except Exception as e:
            self.logger.bind_context(assessment_id, self.name).error(
                f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è: {e}"
            )
            
            end_time = datetime.now()
            execution_time = (end_time - start_time).total_seconds()
            
            return AgentTaskResult(
                agent_name=self.name,
                task_type="profiling",
                status=ProcessingStatus.FAILED,
                error_message=str(e),
                start_time=start_time,
                end_time=end_time,
                execution_time_seconds=execution_time
            )

# ===============================
# –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å LangGraph
# ===============================

def create_profiler_node_function(profiler_agent: ProfilerAgent):
    """
    –°–æ–∑–¥–∞–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é —É–∑–ª–∞ –¥–ª—è LangGraph workflow
    
    Args:
        profiler_agent: –≠–∫–∑–µ–º–ø–ª—è—Ä –ø—Ä–æ—Ñ–∞–π–ª–µ—Ä-–∞–≥–µ–Ω—Ç–∞
        
    Returns:
        –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ LangGraph
    """
    async def profiler_node(state: Dict[str, Any]) -> Dict[str, Any]:
        """–£–∑–µ–ª –ø—Ä–æ—Ñ–∞–π–ª–µ—Ä–∞ –≤ LangGraph workflow"""
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        assessment_id = state.get("assessment_id", "unknown")
        source_files = state.get("source_files", [])
        agent_name = state.get("preliminary_agent_name", "Unknown_Agent")
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        input_data = {
            "source_files": source_files,
            "agent_name": agent_name
        }
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—Ñ–∞–π–ª–µ—Ä
        result = await profiler_agent.run(input_data, assessment_id)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ - –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º AgentTaskResult –≤ —Å–ª–æ–≤–∞—Ä—å
        updated_state = state.copy()
        updated_state["profiling_result"] = result.dict()  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ª–æ–≤–∞—Ä—å
        
        if result.status == ProcessingStatus.COMPLETED:
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Ñ–∏–ª—å –∞–≥–µ–Ω—Ç–∞ –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
            agent_profile_data = result.result_data["agent_profile"]
            updated_state["agent_profile"] = agent_profile_data
            updated_state["current_step"] = "evaluation_preparation"
        else:
            updated_state["current_step"] = "error"
            updated_state["error_message"] = result.error_message
        
        return updated_state
    
    return profiler_node


# ===============================
# –§–∞–±—Ä–∏–∫–∏
# ===============================

def create_profiler_agent(
    llm_base_url: str = "http://127.0.0.1:1234",
    llm_model: str = "qwen3-4b",
    temperature: float = 0.1
) -> ProfilerAgent:
    """
    –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∞–π–ª–µ—Ä-–∞–≥–µ–Ω—Ç–∞
    
    Args:
        llm_base_url: URL LLM —Å–µ—Ä–≤–µ—Ä–∞
        llm_model: –ú–æ–¥–µ–ª—å LLM
        temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        
    Returns:
        –ù–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–π –ø—Ä–æ—Ñ–∞–π–ª–µ—Ä-–∞–≥–µ–Ω—Ç
    """
    from .base_agent import create_agent_config
    
    config = create_agent_config(
        name="profiler_agent",
        description="–ê–≥–µ–Ω—Ç –¥–ª—è –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤ –∏ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–æ–≤",
        llm_base_url=llm_base_url,
        llm_model=llm_model,
        temperature=temperature,
        max_retries=3,
        timeout_seconds=1800,  # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π —Ç–∞–π–º-–∞—É—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        use_risk_analysis_client=False  # –ü—Ä–æ—Ñ–∞–π–ª–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∫–ª–∏–µ–Ω—Ç
    )
    
    return ProfilerAgent(config)


def create_profiler_from_env() -> ProfilerAgent:
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Ñ–∞–π–ª–µ—Ä-–∞–≥–µ–Ω—Ç–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è"""
    import os
    
    return create_profiler_agent(
        llm_base_url=os.getenv("LLM_BASE_URL", "http://127.0.0.1:1234"),
        llm_model=os.getenv("LLM_MODEL", "qwen3-4b"),
        temperature=float(os.getenv("LLM_TEMPERATURE", "0.1"))
    )


# –≠–∫—Å–ø–æ—Ä—Ç
__all__ = [
    "ProfilerAgent",
    "create_profiler_agent",
    "create_profiler_from_env",
    "create_profiler_node_function"
]